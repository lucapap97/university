# JOS: Ordine cronologico del boot

## (1) ```0xffff0```: BIOS

## (2) ```0x7c00```: boot/boot.S

```nasm
gdt:
  SEG_NULL
  SEG(STA_X | STA_R, 0x00000000, 0xFFFFFFFF) ; code segment
  SEG(STA_W, 0x00000000, 0xFFFFFFFF)         ; data segment
gdtdesc:
  .word   0x17 # sizeof(gdt) - 1
  .long   gdt  # address gdt

start:
  ...
  lgdt    gdtdesc
  movl    %cr0, %eax
  orl     $CR0_PE_ON, %eax
  movl    %eax, %cr0
  ljmp    $PROT_MODE_CSEG, $protcseg ; Switches CPU into 32-bit mode.

protcseg:
  ...
  movl    $start, %esp ; %start = 0x7c00
  call    bootmain
```

## (3) cont'd: boot/main.c

```c
void bootmain() {
  ...
  ((void (*)(void)) (ELFHDR->e_entry))();
}
```

## (4) ```0x10000c```: kern/entry.S

```nasm
.data
  .p2align PGSHIFT ; PGSHIFT = 12 (log2(PGSIZE)) where PGSIZE = 4KB
  .globl bootstack
bootstack:
  .space KSTKSIZE ; KSTKSIZE = 8 * PGSIZE = 8 * 4KB = 32KB stack
  .globl bootstacktop
bootstacktop:

entry:
  movl   $(RELOC(entry_pgdir)), %eax
  movl   %eax, %cr3

  movl   %cr0, %eax
  orl    $(CR0_PE|CR0_PG|CR0_WP), %eax
  movl   %eax, %cr0

  mov    $relocated, %eax ; still low address
  jmp    *%eax            ; still low address

relocated:                ; high address (above KERNBASE)
  ...
  movl    $(bootstacktop), %esp
  call    i386_init
```

## (4.1) cont'd: kern/entrypgdir.c

```c
// NPDENTRIES = 1024 (page directory/table entries per page directory/table)
pde_t entry_pgdir[NPDENTRIES] = {
  // map VA [0, 4MB) to PA [0, 4MB)
  [0] = ((uintptr_t) entry_pgtable - KERNBASE) + PTE_P,

  // Map VA [KERNBASE, KERNBASE  +4MB) to PA [0, 4MB)
  [KERNBASE >> PDXSHIFT] = ((uintptr_t)entry_pgtable - KERNBASE) + PTE_P + PTE_W
  // where KERNBASE = 0xF0000000
  // where PDXSHIFT = 22
  // [960] = [0x3c0]
};

pte_t entry_pgtable[NPTENTRIES] = {
  0x000000 | PTE_P | PTE_W,
  0x001000 | PTE_P | PTE_W,
  ...
  0x3fe000 | PTE_P | PTE_W,
  0x3ff000 | PTE_P | PTE_W,
};
```

## (5) cont'd: kern/init.c

```c
void i386_init(void) {
  mem_init();
  ...
  trap_init();
}
```

## (5.1) cont'd: kern/pmap.c

```c
struct PageInfo {
  struct PageInfo *pp_link;
  uint16_t pp_ref;
};

void mem_init(void) {
  i386_detect_memory();

  kern_pgdir = (pde_t *) boot_alloc(PGSIZE);
  memset(kern_pgdir, 0, PGSIZE);

  // where npages is the number of free pages (free memory)
  pages = (struct PageInfo *) boot_alloc(npages * sizeof(struct PageInfo));
  memset(pages, 0, npages * sizeof(struct PageInfo));

  envs = (struct Env *)boot_alloc(NENV * sizeof(struct Env));
  memset(envs, 0, NENV * sizeof(struct Env));

  page_init();

  boot_map_region(
    kern_pgdir, // page directory
    KERNBASE,   // virtual address 0xF0000000
    ROUNDUP((0xFFFFFFFF) - KERNBASE + 1, PGSIZE), // size
    0x00000000, // physical address
    PTE_W | PTE_P); // permissions

  boot_map_region(
    kern_pgdir, // page directory
    UPAGES,     // virtual address 0xEF000000
    ROUNDUP(sizeof(struct PageInfo) * npages, PGSIZE),
    PADDR(pages),
    PTE_U);

  boot_map_region(
    kern_pgdir,
    UENVS,      // virtual address 0xEEC00000
    ROUNDUP(sizeof(struct Env) * NENV, PGSIZE),
    PADDR(envs),
    PTE_U);

  boot_map_region(
    kern_pgdir,
    KSTACKTOP - KSTKSIZE, // where KSTACKTOP = 0xEFC00000,
    KSTKSIZE,             // where KSTKSIZE = 8 * PGSIZE = 8 * 4KB = 32KB
    PADDR(bootstack),
    PTE_W);

  lcr3(PADDR(kern_pgdir));

  check_page_free_list(0); // reverse page_free_list order

  // CR0_PE = protection enabled
  // CR0_PG = paging enabled
  // CR0_AM = alignment mask
  // CR0_WP = write protect
  // CR0_NE = numeric error
  // CR0_MP = monitor coprocessor
  // CR0_TS = task switched
  // CR0_EM = emulation
  cr0 = rcr0();
  cr0 |= CR0_PE | CR0_PG | CR0_AM | CR0_WP | CR0_NE | CR0_MP;
  cr0 &= ~(CR0_TS | CR0_EM);
  lcr0(cr0);
}

static void *boot_alloc(uint32_t n) { // number of bytes to alloc  static char *nextfree; // virtual address of next byte of free memory
  if (!nextfree) {
    // 'end' is a magic symbol automatically generated by the linker,
    // which points to the end of the kernel's bss segment:
    // the first virtual address that the linker did not assign
    // to any kernel code or global variables.
    extern char end[];
    nextfree = ROUNDUP((char *) end, PGSIZE);
  }

  char *result = nextfree;
  if (n > 0) { // i want to alloc memory
    uint32_t alloc_size = ROUNDUP(n, PGSIZE);
    nextfree += alloc_size

    if ((uintptr_t) nextfree >= 0xf0400000) {
      panic("boot_alloc: out of memory");
    }
  } // else just return nextfree

  return result;
}

void page_init(void) {
  for (size_t i = 0; i < npages; i++) {
    physaddr_t pa = page2pa(&pages[i]);
    char *va = page2kva(&pages[i]);

    // at IOPHYSMEM (640K = 0xA0000) there is a 384K hole for I/O (LOW MEM)
    if (i == 0
        || (IOPHYSMEM <= pa && va < (char *)boot_alloc(0))
        || (MPENTRY_PADDR <= pa && pa < MPENTRY_PADDR + PGSIZE)) {
      pages[i].pp_ref = 1;
      pages[i].pp_link = NULL;
      // used pages, both mapped and already in use
    } else {
      pages[i].pp_ref = 0;
      pages[i].pp_link = page_free_list;
      page_free_list = &pages[i];
      // free pages
    }
  }
}

struct PageInfo *page_alloc(int alloc_flags) {
  if (page_free_list == NULL) {
    return NULL;
  }

  struct PageInfo *allocated_page = page_free_list;
  allocated_page->pp_link = NULL;
  if (alloc_flags & ALLOC_ZERO) {
    memset(page2kva(page_free_list), 0, PGSIZE);
  }

  page_free_list = page_free_list->pp_link;
  return allocated_page;
}

void page_free(struct PageInfo *pp) {
  if (pp->pp_ref == 0) {
    memset(page2kva(pp), 0, PGSIZE);
    pp->pp_link = page_free_list;
    page_free_list = pp;
  }
}

void page_decref(struct PageInfo* pp) {
  pp->pp_ref -= 1;
  if (pp->pp_ref == 0) {
    page_free(pp);
  }
}

struct PageInfo *page_lookup(pde_t *pgdir, void *va, pte_t **pte_store) {
  pte_t *pte = pgdir_walk(pgdir, va, 0);
  if (!pte || !(*pte & PTE_P)) { // if pte doesn't exist or is not present
    return NULL;   
  }

  if (pte_store) { // save pte
    *pte_store = pte;
  }

  return pa2page(PTE_ADDR(*pte));
}

int page_insert(pde_t *pgdir, struct PageInfo *pp, void *va, int perm) {
  pte_t *pte = pgdir_walk(pgdir, va, 1);
  if (pte == NULL) {
    return -E_NO_MEM;
  }

  perm &= 0xFFF;
  if (*pte & PTE_P) {
    if (PTE_ADDR(*pte) != page2pa(pp)) { // present but different mapping
      page_remove(pgdir, va); // remove old mapping
      *pte = page2pa(pp) | perm | PTE_P;
      pp->pp_ref += 1;
      tlb_invalidate(pgdir, va);
    } else { // present but same mapping, just change the permission
      *pte = (*pte & 0xFFFFF000) | perm | PTE_P;
    }
  } else { // not present at all, add it
    *pte = page2pa(pp) | perm | PTE_P;
    pp->pp_ref += 1;
  }

  return 0;
}

void page_remove(pde_t *pgdir, void *va) {
  pte_t *pt_element;  
  struct PageInfo *page_to_remove = page_lookup(pgdir, va, &ptelement);
  if (page_to_remove == NULL) {
    return;
  }

  *ptelement = 0;
  page_decref(page_to_remove);
  tlb_invalidate(pgdir, va);
  return;
}

pte_t *pgdir_walk(pde_t *pgdir, const void *va, int create) {
  struct PageInfo *new_page;
  if ((pgdir[PDX(va)] & PTE_P) != 0) { // if present return its address
    return &(((pte_t *)KADDR(PTE_ADDR(pgdir[PDX(va)])))[PTX(va)]);
  } else {
    if (create == 0) {
      return NULL;
    } else { // else create a new one and return its address
      new_page = page_alloc(ALLOC_ZERO);
      if (new_page == NULL) {
        return NULL;
      }
      new_page->pp_ref = 1;

      pgdir[PDX(va)] = page2pa(new_page) | PTE_P | PTE_W | PTE_U;
      return &(((pte_t *)KADDR(PTE_ADDR(pgdir[PDX(va)])))[PTX(va)]);
    }
  }
}

static void boot_map_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm) {
  for (size_t i = 0; i < size; i += PGSIZE) {
    pte_t *pte = pgdir_walk(pgdir, (void *)(va + i), 1);
    *pte = (pa + i) | perm | PTE_P;
  }
}

// where PGSHIFT = 12
physaddr_t page2pa(struct PageInfo *pp) {
  return (pp - pages) << PGSHIFT;
}

void* page2kva(struct PageInfo *pp) {
  return KADDR(page2pa(pp));
}
```

## (6) kern/trap.c

```c
struct Segdesc gdt[NCPU + 5] = {
  // 0x0 - unused (always faults -- for trapping NULL far pointers)
  SEG_NULL,
  // 0x8 - kernel code segment
  [GD_KT >> 3] = SEG(STA_X | STA_R, 0x0, 0xffffffff, 0),

  // 0x10 - kernel data segment
  [GD_KD >> 3] = SEG(STA_W, 0x0, 0xffffffff, 0),

  // 0x18 - user code segment
  [GD_UT >> 3] = SEG(STA_X | STA_R, 0x0, 0xffffffff, 3),

  // 0x20 - user data segment
  [GD_UD >> 3] = SEG(STA_W, 0x0, 0xffffffff, 3),

  // Per-CPU TSS descriptors (starting from GD_TSS0) are initialized
  // in trap_init_percpu()
  [GD_TSS0 >> 3] = SEG_NULL
};

struct Gatedesc idt[256] = {
  { 0 },
};

struct Pseudodesc idt_pd = {
  sizeof(idt) - 1,
  (uint32_t) idt
};

void trap_init(void) {
  extern void handler0();
  extern void handler0();
  ...
  extern void handler13();
  extern void handler14();

  extern void irq0_entry();
  extern void irq1_entry();
  ...
  extern void irq13_entry();
  extern void irq14_entry();

  // where SETGATE(gate, istrap, selector, offset, dpl)
  SETGATE(idt[T_DIVIDE], 0, GD_KT, handler0, 0);
  SETGATE(idt[T_DEBUG], 0, GD_KT, handler1, 0);
  ...
  SETGATE(idt[T_PGFLT], 0, GD_KT, handler14, 0);
  SETGATE(idt[T_FPERR], 0, GD_KT, handler16, 0);

  trap_init_percpu();
}

void trap_init_percpu() {
  thiscpu->cpu_ts.ts_esp0 = KSTACKTOP - cpunum() * (KSTKSIZE + KSTKGAP);
  ts->cpu_ts.ts_ss0 = GD_KD;

  // Initialize the TSS slot of the gdt.
  gdt[(GD_TSS0 >> 3) + cpunum()] =
    SEG16(STS_T32A, (uint32_t) (&thiscpu->cpu_ts), sizeof(struct Taskstate), 0);

  gdt[(GD_TSS0 >> 3) + cpunum()].sd_s = 0;

  // Load the TSS selector (like other segment selectors, the
  // bottom three bits are special; we leave them 0)
  ltr(GD_TSS0 + (cpunum() << 3));

  // Load the IDT
  lidt(&idt_pd);
}

void trap(struct Trapframe *tf) {
  // where 0x11 = USER MODE
  if ((tf->tf_cs & 0x11) == 0x11) { // if trapped from user mode
    assert(curenv);
    curenv->env_tf = *tf;
    tf = &(curenv->env_tf);
  }
  trap_dispatch(tf);
}

static void trap_dispatch(struct Trapframe *tf) {
  if (tf->tf_trapno == IRQ_OFFSET + IRQ_TIMER) {
    lapic_eoi();
    sched_yield();
    return;
  }

  if (tf->tf_cs == GD_KT) {
    panic("unhandled trap in kernel");
  }

  switch (tf->tf_trapno) {
    case T_PGFLT:
      page_fault_handler(tf);
      break;
    case T_BRKPT:
      monitor(tf);
      break;
    case T_SYSCALL:
      tf->tf_regs.reg_eax = syscall(
        tf->tf_regs.reg_edx,
        tf->tf_regs.reg_ecx,
        tf->tf_regs.reg_ebx,
        tf->tf_regs.reg_edi,
        tf->tf_regs.reg_esi);
      return;
    default:
      env_destroy(curenv);
      return;
  }

  if (curenv && curenv->env_status == ENV_RUNNING) {
    env_run(curenv);
  } else {
    sched_yield();  
  }
}
```

## inc/mmu.h

```c
// Task state segment format (as described by the Pentium architecture book)
struct Taskstate {
    ...
    uintptr_t ts_esp0;   // Stack pointers and segment selectors
    uint16_t ts_ss0;     // after an increase in privilege level
    ...
    physaddr_t ts_cr3;   // Page directory base
    uintptr_t ts_eip;    // Saved state from last task switch
    uint32_t ts_eflags;
    uint32_t ts_eax;     // More saved state (registers)
    uint32_t ts_ecx;
    uint32_t ts_edx;
    uint32_t ts_ebx;
    uintptr_t ts_esp;
    uintptr_t ts_ebp;
    uint32_t ts_esi;
    uint32_t ts_edi;
    uint16_t ts_es;      // Even more saved state (segment selectors)
    ...
    uint16_t ts_t;       // Trap on task switch
    uint16_t ts_iomb;    // I/O map base address
};

#define SETGATE(gate, istrap, sel, off, dpl) {
  (gate).gd_off_15_0 = (uint32_t) (off) & 0xffff;
  (gate).gd_sel = (sel);
  (gate).gd_args = 0;
  (gate).gd_rsv1 = 0;
  (gate).gd_type = (istrap) ? STS_TG32 : STS_IG32;
  (gate).gd_s = 0;
  (gate).gd_dpl = (dpl);
  (gate).gd_p = 1;
  (gate).gd_off_31_16 = (uint32_t) (off) >> 16;
}
```

## kern/trapentry.S

```nasm
/* TRAPHANDLER defines a globally-visible function for handling a trap.
 * It pushes a trap number onto the stack, then jumps to _alltraps.
 * Use TRAPHANDLER for traps where the CPU automatically pushes an error
 * code.
 */

#define TRAPHANDLER(name, num)
  .globl name
  .type name, @function
  .align 2
  name:
    pushl $(num)
    jmp _alltraps

/* Use TRAPHANDLER_NOEC for traps where the CPU doesn't push an error
 * code. It pushes a 0 in place of the error code, so the trap frame
 * has the same format in either case.
 */

#define TRAPHANDLER_NOEC(name, num)
    .globl name
    .type name, @function
    .align 2
    name:
      pushl $0
      pushl $(num)
      jmp _alltraps

.text

TRAPHANDLER_NOEC(handler0, T_DIVIDE);
TRAPHANDLER_NOEC(handler1, T_DEBUG);
..
TRAPHANDLER(handler13, T_GPFLT);
TRAPHANDLER(handler14, T_PGFLT);
TRAPHANDLER_NOEC(handler16, T_FPERR);
TRAPHANDLER_NOEC(irq0_entry, IRQ_OFFSET + 0);
TRAPHANDLER_NOEC(irq1_entry, IRQ_OFFSET + 1);
...
TRAPHANDLER_NOEC(irq13_entry, IRQ_OFFSET + 13);
TRAPHANDLER_NOEC(irq14_entry, IRQ_OFFSET + 14);

_alltraps:
  push %ds
  push %es
  pushal

  mov $GD_KD, %ax
  mov %ax, %ds
  mov %ax, %es

  pushl %esp
  call trap

.data
.globl handlers
handlers:
  .long handler0
  .long handler1
  ...
  .long handler254
  .long handler255
```
